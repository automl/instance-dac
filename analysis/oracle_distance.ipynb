{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from rich import print as printr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from instance_dac.utils.data_loading import load_performance_data, load_eval_data\n",
    "\n",
    "# LEVELS OF AGGREGATION\n",
    "# 1. eval episode\n",
    "# 2. seed\n",
    "# 3. instance\n",
    "\n",
    "# data.csv missing? Run\n",
    "path = \"../runs/Sigmoid\"\n",
    "agent_name = \"ppo\"\n",
    "train_instance_set = \"2D3M_train\"\n",
    "train_instance_set_id = \"sigmoid_2D3M_train\"\n",
    "test_instance_set = \"2D3M_test\"\n",
    "test_instance_set_id = \"sigmoid_2D3M_test\"\n",
    "benchmark_id = \"Sigmoid\"\n",
    "\n",
    "path = \"../runs/CMA-ES\"\n",
    "agent_name = \"ppo_sb3\"\n",
    "train_instance_set = \"seplow_train\"\n",
    "test_instance_set_id = \"test\"\n",
    "train_instance_set_id = \"train\"\n",
    "test_instance_set = \"seplow_test\"\n",
    "benchmark_id = \"CMA-ES\"\n",
    "\n",
    "aggregate_selector_runs = True\n",
    "normalize_performance_per_instance = True\n",
    "\n",
    "path = Path(path) / train_instance_set / agent_name\n",
    "# data = load_eval_data(path=path, instance_set_id=test_instance_set_id, instance_set=test_instance_set)\n",
    "\n",
    "data = pd.read_csv(f\"eval_data_{test_instance_set_id}.csv\")\n",
    "\n",
    "data= data.drop(np.where(data[\"origin\"] == \"oracle\")[0])\n",
    "\n",
    "ids = data[\"origin\"] == \"selector\"\n",
    "data[\"origin\"][ids] = data[\"instance_set_id\"][ids].apply(lambda x: f\"selector_{x}\")\n",
    "\n",
    "if normalize_performance_per_instance:\n",
    "    D = []\n",
    "    for gid, gdf in data.groupby(\"instance\"):\n",
    "        Gmax = gdf[\"overall_performance\"].max()\n",
    "        Gmin = gdf[\"overall_performance\"].min()\n",
    "        gdf[\"overall_performance\"] = (gdf[\"overall_performance\"] - Gmin) / (Gmax - Gmin)\n",
    "        D.append(gdf)\n",
    "    data = pd.concat(D).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gid, gdf in data.groupby(\"origin\"):\n",
    "#     if gdf[\"instance\"].nunique() > 10:\n",
    "#         printr(gid)\n",
    "#         printr(gdf[\"instance\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = data[\"origin\"].str.startswith(\"selector\")\n",
    "# Prepare to aggregate across runs and seeds\n",
    "if aggregate_selector_runs:\n",
    "    data.loc[ids, \"origin\"] = data[\"instance_set_id\"][ids].apply(lambda x: \"selector__\" + \"__\".join(x.split(\"__\")[:-2]))\n",
    "else:\n",
    "    data.loc[ids, \"origin\"] = data[\"instance_set_id\"][ids].apply(lambda x: \"selector__\" + \"__\".join(x.split(\"__\")[:-2]) + f\"__{x.split('__')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fanova\n",
    "import fanova.visualizer\n",
    "from ConfigSpace import ConfigurationSpace, Categorical, OrdinalHyperparameter\n",
    "\n",
    "import logging\n",
    "import itertools as it\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ConfigSpace.hyperparameters import Hyperparameter, CategoricalHyperparameter, Constant, OrdinalHyperparameter, \\\n",
    "    NumericalHyperparameter\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set(font_scale=2)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "\n",
    "df = data[data[\"origin\"].str.startswith(\"selector\")]\n",
    "df.loc[:, \"method\"] = df[\"instance_set_id\"].apply(lambda x: x.split(\"__\")[2])\n",
    "df.loc[:, \"feature_type\"] = df[\"instance_set_id\"].apply(lambda x: x.split(\"__\")[3])\n",
    "df.loc[:, \"source_features\"] = df[\"instance_set_id\"].apply(lambda x: x.split(\"__\")[4])\n",
    "df.loc[:, \"threshold\"] = df[\"instance_set_id\"].apply(lambda x: float(x.split(\"__\")[5]))\n",
    "\n",
    "cols = [\"method\", \"feature_type\", \"source_features\", \"threshold\"]\n",
    "cols.sort()\n",
    "cat_features = [\"method\", \"feature_type\", \"source_features\"]\n",
    "cat_features.sort()\n",
    "\n",
    "cat_map = {k: list(df[k].unique()) for k in cat_features}\n",
    "printr(cat_map)\n",
    "for k, v in cat_map.items():\n",
    "    df[k] = df[k].apply(lambda x: v.index(x))\n",
    "\n",
    "\n",
    "sequence = list(df[\"threshold\"].unique())\n",
    "sequence.sort()\n",
    "config_space = ConfigurationSpace()\n",
    "hps = [\n",
    "    Categorical(name=\"method\", items=df[\"method\"].unique()),\n",
    "    Categorical(name=\"feature_type\", items=df[\"feature_type\"].unique()),\n",
    "    Categorical(name=\"source_features\", items=df[\"source_features\"].unique()),\n",
    "    OrdinalHyperparameter(name=\"threshold\", sequence=sequence),\n",
    "    \n",
    "]\n",
    "config_space.add_hyperparameters(hps)\n",
    "\n",
    "\n",
    "\n",
    "printr(config_space.get_hyperparameters())\n",
    "\n",
    "X = df.loc[:, cols]\n",
    "y = df.loc[:, \"overall_performance\"].to_numpy()\n",
    "\n",
    "f = fanova.fANOVA(X, y, config_space = config_space, n_trees=32, bootstrapping=True)\n",
    "\n",
    "print(f.trees_total_variance)\n",
    "print(f.V_U_individual)\n",
    "\n",
    "dims = (0,1,2,3)  # idx of configspace HPs\n",
    "vlines = [\n",
    "    [0.05, 0.1, 0.5, 1],\n",
    "    [0, 1, 2],\n",
    "    [0.05, 0.1, 0.25, 1],\n",
    "]\n",
    "res = f.quantify_importance(dims)\n",
    "print(res)\n",
    "\n",
    "importance_dict = f.quantify_importance([0,1,2])    \n",
    "\n",
    "for k in sorted(list(importance_dict.keys()), key=lambda t: importance_dict[t]['individual importance'], reverse=True):\n",
    "    print(k, importance_dict[k])\n",
    "\n",
    "v = fanova.visualizer.Visualizer(f, f.cs, \".\")\n",
    "\n",
    "\n",
    "def plot_marginal(self, param, resolution=100, log_scale=None, show=True, incumbents=None, ax=None):\n",
    "    \"\"\"\n",
    "    Creates a plot of marginal of a selected parameter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    param: int or str\n",
    "        Index of chosen parameter in the ConfigSpace (starts with 0)\n",
    "    resolution: int\n",
    "        Number of samples to generate from the parameter range as values to predict\n",
    "    log_scale: boolean\n",
    "        If log scale is required or not. If no value is given, it is deduced from the ConfigSpace provided\n",
    "    show: boolean\n",
    "        whether to call plt.show() to show plot directly as interactive matplotlib-plot\n",
    "    incumbents: List[Configuration]\n",
    "        list of ConfigSpace.Configurations that are marked as incumbents\n",
    "    \"\"\"\n",
    "    param, param_name, param_idx = self._get_parameter(param)\n",
    "    print(param, self._get_parameter(param))\n",
    "\n",
    "    # check if categorical\n",
    "    if isinstance(param, NumericalHyperparameter):\n",
    "        # PREPROCESS\n",
    "        mean, std, grid = self.generate_marginal(param_idx, resolution)\n",
    "        mean = np.asarray(mean)\n",
    "        std = np.asarray(std)\n",
    "\n",
    "        lower_curve = mean - std\n",
    "        upper_curve = mean + std\n",
    "\n",
    "        if log_scale is None:\n",
    "            log_scale = param.log or (np.diff(grid).std() > 0.000001)\n",
    "\n",
    "        # PLOT\n",
    "        if log_scale:\n",
    "            if np.diff(grid).std() > 0.000001:\n",
    "                self.logger.info(\"It might be better to plot this parameter '%s' in log-scale.\", param_name)\n",
    "            plt.semilogx(grid, mean, 'b', label='predicted %s' % self._y_label)\n",
    "        else:\n",
    "            plt.plot(grid, mean, 'b', label='predicted %s' % self._y_label)\n",
    "        plt.fill_between(grid, upper_curve, lower_curve, facecolor='red', alpha=0.6, label='std')\n",
    "\n",
    "        if incumbents is not None:\n",
    "            if not isinstance(incumbents, list):\n",
    "                incumbents = [incumbents]\n",
    "            values = [inc[param_name] for inc in incumbents if param_name in inc and inc[param_name] is not None]\n",
    "            indices = [(np.abs(np.asarray(grid) - val)).argmin() for val in values]\n",
    "            if len(indices) > 0:\n",
    "                plt.scatter(list([grid[idx] for idx in indices]),\n",
    "                            list([mean[idx] for idx in indices]),\n",
    "                            label='incumbent', c='black', marker='.', zorder=999)\n",
    "\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel(self._y_label)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "    else:\n",
    "        # PREPROCESS\n",
    "        if isinstance(param, CategoricalHyperparameter):\n",
    "            labels = param.choices\n",
    "            categorical_size = len(param.choices)\n",
    "        elif isinstance(param, OrdinalHyperparameter):\n",
    "            labels = param.sequence\n",
    "            categorical_size = len(param.sequence)\n",
    "        elif isinstance(param, Constant):\n",
    "            labels = str(param)\n",
    "            categorical_size = 1\n",
    "        else:\n",
    "            raise ValueError(\"Parameter %s of type %s not supported.\" % (param.name, type(param)))\n",
    "\n",
    "        indices = np.arange(1, categorical_size + 1, 1)\n",
    "        mean, std = self.generate_marginal(param_idx)\n",
    "        min_y = mean[0]\n",
    "        max_y = mean[0]\n",
    "\n",
    "        # PLOT\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "        b = ax.boxplot(\n",
    "            [[x] for x in mean], \n",
    "            patch_artist=False, \n",
    "            showmeans=False, \n",
    "            showfliers=False,\n",
    "            medianprops={\n",
    "                \"color\": \"darkslateblue\", \n",
    "                \"linewidth\": 5\n",
    "            },\n",
    "            boxprops={\n",
    "                # \"facecolor\": \"C0\", \n",
    "                # \"edgecolor\": \"white\",\n",
    "                \"color\": \"darkmagenta\",\n",
    "                \"linewidth\": 5,\n",
    "            },\n",
    "            # whiskerprops={\"color\": \"C0\", \"linewidth\": 1.5},\n",
    "            # capprops={\"color\": \"C0\", \"linewidth\": 1.5}\n",
    "        )\n",
    "        ax.set_xticks(indices, labels)\n",
    "\n",
    "        # blow up boxes\n",
    "        for box, std_ in zip(b[\"boxes\"], std):\n",
    "            y = box.get_ydata()\n",
    "            y[2:4] = y[2:4] + std_\n",
    "            y[0:2] = y[0:2] - std_\n",
    "            y[4] = y[4] - std_\n",
    "            box.set_ydata(y)\n",
    "            min_y = min(min_y, y[0] - std_)\n",
    "            max_y = max(max_y, y[2] + std_)\n",
    "\n",
    "        ax.set_ylim([min_y, max_y])\n",
    "\n",
    "        # _bp = plt.boxplot([[x] for x in mean], patch_artist=True)\n",
    "        # for median in _bp['medians']: median.set(color ='forestgreen', linewidth = 3, alpha=0.4) \n",
    "        # for cap in _bp['caps']: cap.set(color ='#8B008B', linewidth = 2) \n",
    "\n",
    "        ax.set_ylabel(self._y_label)\n",
    "        ax.set_xlabel(param_name)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return ax\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig = plt.figure(figsize=(len(dims)*6, 4), dpi=300)\n",
    "axes = fig.subplots(nrows=1, ncols=len(dims), sharey=False)\n",
    "for i, dim in enumerate(dims):\n",
    "    ax = axes[i]\n",
    "    ax = plot_marginal(v, dim, show=False, ax=ax)\n",
    "    # ax = plt.gca()\n",
    "    # x = vlines[dim]\n",
    "    # ymin, ymax = plt.gca().get_ylim()\n",
    "    # plt.vlines(x, ymin, ymax)\n",
    "    print(dim, cols[dim])\n",
    "    if cols[dim] in cat_features:\n",
    "        ax.set_xticklabels(cat_map[cols[dim]], fontsize=19)\n",
    "    # match cols[dim]:\n",
    "    #     case \"method\":\n",
    "    #         ax.set_xticks(np.arange(0, len(cat_map[\"method\"])))  # hack\n",
    "    #         ax.set_xticklabels(cat_map[\"method\"], fontsize=19)\n",
    "    #     case \"delta\":\n",
    "    #         ax.set_xlabel(r\"$\\Delta \\alpha$\")\n",
    "    #     case \"atol_rel\":\n",
    "    #         ax.set_xlabel(r\"$\\epsilon$\")\n",
    "    ax.set_ylabel(None)\n",
    "axes[0].set_ylabel(\"performance (normalized)\")\n",
    "fig.set_tight_layout(False)\n",
    "fn = Path(f\"figures/HP_importance/{benchmark_id}.pdf\")\n",
    "fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(fn, bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()\n",
    "    \n",
    "# v.plot_pairwise_marginal([0,1], resolution=50)\n",
    "# v.create_all_plots()\n",
    "# v.create_most_important_pairwise_marginal_plots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"origin\"].unique()\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(data[data[\"origin\"] != \"random\"].groupby([\"origin\", \"seed\"])[\"overall_performance\"].mean())\n",
    "tmp.to_csv(\"tmp.csv\")\n",
    "printr(tmp.max())\n",
    "printr(list(tmp.idxmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate across eval episodes\n",
    "if aggregate_selector_runs:\n",
    "    D = []\n",
    "    for gid, gdf in data.groupby([\"origin\", \"seed\", \"instance\", \"selector_run\"]):\n",
    "        if gid[0].startswith(\"selector\"):\n",
    "            D.append(pd.Series({\n",
    "                \"origin\": gid[0],\n",
    "                \"seed\": gid[1],\n",
    "                \"instance\": gid[2],\n",
    "                \"overall_performance\": gdf[\"overall_performance\"].mean()\n",
    "            }))\n",
    "        else:\n",
    "            print(gid)\n",
    "            D.append(gdf)\n",
    "    perf = pd.concat(D, axis=1).T\n",
    "    perf = pd.concat([perf, data[~data[\"origin\"].str.startswith(\"selector\")]])\n",
    "else:\n",
    "    perf = data\n",
    "\n",
    "perf = pd.DataFrame(perf.groupby([\"origin\", \"seed\", \"instance\"])[\"overall_performance\"].mean())\n",
    "perf_dict = {}\n",
    "for gid, gdf in perf.groupby(\"origin\"):\n",
    "    gdf = gdf.reset_index()\n",
    "    P = gdf[\"overall_performance\"].to_numpy()\n",
    "    P = P.reshape((gdf[\"seed\"].nunique(), gdf[\"instance\"].nunique()))\n",
    "    perf_dict[gid] = P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_fun = np.mean\n",
    "sorted_series = data.groupby([\"origin\"]).apply(lambda x: agg_fun(x[\"overall_performance\"])).sort_values()\n",
    "sorter = list(sorted_series.index)\n",
    "\n",
    "df = data.sort_values(by=\"origin\", key=lambda column: column.map(lambda e: sorter.index(e)))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "df[\"-overall_performance\"] = -df[\"overall_performance\"]\n",
    "ax = sns.boxplot(data=df, x=\"origin\", y=\"overall_performance\", ax=ax, fliersize=1, showfliers=True, showmeans=True)\n",
    "ax.tick_params(axis='x', labelrotation=90, labelsize=5)\n",
    "# ax.set_yscale(\"log\")\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax = sns.boxplot(data=df, x=\"instance\", y=\"-overall_performance\", ax=ax)\n",
    "# ax.tick_params(axis='x', labelrotation=90, labelsize=5)\n",
    "# ax.set_yscale(\"log\")\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax = sns.boxplot(data=df, x=\"seed\", y=\"-overall_performance\", ax=ax)\n",
    "# ax.tick_params(axis='x', labelrotation=90, labelsize=5)\n",
    "# ax.set_yscale(\"log\")\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax = sns.boxplot(data=df, x=\"selector_run\", y=\"-overall_performance\", ax=ax)\n",
    "# ax.tick_params(axis='x', labelrotation=90, labelsize=5)\n",
    "# ax.set_yscale(\"log\")\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rliable import library as rly\n",
    "from rliable import metrics\n",
    "from rliable import plot_utils\n",
    "\n",
    "metric_names = ['Median', 'IQM', 'Mean']#, 'Optimality Gap']\n",
    "\n",
    "algorithms = list(perf_dict.keys())\n",
    "# Load ALE scores as a dictionary mapping algorithms to their human normalized\n",
    "# score matrices, each of which is of size `(num_runs x num_games)`.\n",
    "aggregate_func = lambda x: np.array([\n",
    "  metrics.aggregate_median(x),\n",
    "  metrics.aggregate_iqm(x),\n",
    "  metrics.aggregate_mean(x),\n",
    "  #metrics.aggregate_optimality_gap(x)\n",
    "  ])\n",
    "aggregate_scores, aggregate_score_cis = rly.get_interval_estimates(\n",
    "  perf_dict, aggregate_func, reps=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'perf_dict_{benchmark_id}.pickle', 'wb') as handle:\n",
    "    pickle.dump(perf_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'aggregate_scores_{benchmark_id}.pickle', 'wb') as handle:\n",
    "    pickle.dump(aggregate_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'aggregate_score_cis_{benchmark_id}.pickle', 'wb') as handle:\n",
    "    pickle.dump(aggregate_score_cis, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_selector = \"selector_2D3M_train__Train__DS__Raw__A__0.7__9__2\"\n",
    "# algos = [a for a in algorithms if a in [\"full\", \"oracle\", \"random\", best_selector]]\n",
    "\n",
    "fig, axes = plot_utils.plot_interval_estimates(\n",
    "  aggregate_scores, aggregate_score_cis,\n",
    "  metric_names=metric_names,\n",
    "  algorithms=algorithms, xlabel='Performance')\n",
    "fig.savefig(f\"plot_{benchmark_id}.pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = []\n",
    "for key, value in aggregate_scores.items():\n",
    "    d = {\n",
    "        \"origin\": key,\n",
    "    }\n",
    "    d.update({m: v for m, v in zip(metric_names, value)})\n",
    "    D.append(d)\n",
    "df_scores = pd.DataFrame(D)\n",
    "\n",
    "# Highest IQM\n",
    "printr(\"Highest IQM\")\n",
    "ind = np.argpartition(df_scores[\"IQM\"], -2)[-4:]  # select 2nd best because best is oracle\n",
    "printr(df_scores.iloc[ind])\n",
    "printr(df_scores.iloc[ind][\"origin\"])\n",
    "\n",
    "# IQM Full\n",
    "printr(\"IQM full\")\n",
    "printr(df_scores[df_scores[\"origin\"] == \"full\"].iloc[0])\n",
    "\n",
    "\n",
    "# Sort by IQM\n",
    "df_scores = df_scores.sort_values(by=\"IQM\")\n",
    "\n",
    "df_scores.to_csv(f\"scores_{benchmark_id}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_list = np.linspace(1, 5, 21)\n",
    "score_distributions, score_distributions_cis = rly.create_performance_profile(\n",
    "    perf_dict, tau_list=tau_list)\n",
    "# Plot score distributions\n",
    "fig, ax = plt.subplots(ncols=1, figsize=(7, 5))\n",
    "plot_utils.plot_performance_profiles(\n",
    "  score_distributions, tau_list=tau_list,\n",
    "  performance_profile_cis=score_distributions_cis,\n",
    "  # colors=dict(zip(algorithms, sns.color_palette('colorblind'))),\n",
    "  xlabel=r'Overall Performance $(\\tau)$',\n",
    "  ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = pd.DataFrame(data.groupby([\"origin\", \"instance\"])[\"overall_performance\"].mean())\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def l1_dist(a, b) -> float:\n",
    "    return a - b \n",
    "\n",
    "def l2_dist(a, b) -> float:\n",
    "    return (a - b) ** 2\n",
    "\n",
    "def calc_dist(x: pd.DataFrame, distance_function) -> pd.DataFrame:\n",
    "    x = x.reset_index()\n",
    "    instance_id = x[\"instance\"][0]\n",
    "    origins = x[\"origin\"]\n",
    "    groundtruth = \"oracle\"\n",
    "    idx_gt = list(origins).index(groundtruth)\n",
    "    dist = distance_function(x[\"overall_performance\"][idx_gt], x[\"overall_performance\"])\n",
    "    comparison_names = [f\"{groundtruth} - {origin}\" for origin in origins]\n",
    "    ret = pd.DataFrame({\n",
    "        \"instance\": instance_id,\n",
    "        \"distance_name\": distance_function.__name__,\n",
    "        \"distance\": dist,\n",
    "        \"compared\": comparison_names\n",
    "    })\n",
    "    return ret\n",
    "\n",
    "distance_functions=[l1_dist, l2_dist]\n",
    "\n",
    "# Compute distance between oracle performance and performance on full training set\n",
    "diffs_per_instance = pd.concat([perf.groupby(\"instance\").apply(calc_dist, distance_function=func) for func in distance_functions], axis=0).reset_index(drop=True)\n",
    "diffs_per_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort df by agg_fun\n",
    "agg_fun = np.mean\n",
    "\n",
    "sorted_series = diffs_per_instance.groupby([\"compared\"]).apply(lambda x: agg_fun(x[\"distance\"])).sort_values()\n",
    "sorter = list(sorted_series.index)\n",
    "\n",
    "df = diffs_per_instance.sort_values(by=\"compared\", key=lambda column: column.map(lambda e: sorter.index(e)))\n",
    "df = df[df[\"distance_name\"] == \"l2_dist\"]\n",
    "\n",
    "fig = plt.figure(figsize=(12,5), dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax = sns.boxplot(data=df, x=\"compared\", y=\"distance\", ax=ax, fliersize=2)\n",
    "ax.tick_params(axis='x', labelrotation=90, labelsize=5)\n",
    "# ax.set_ylim(0, ax.get_ylim()[1])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(12,5), dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax = sns.barplot(data=df, x=\"compared\", y=\"distance\", ax=ax) #, err_kws={\"color\": \".5\", \"linewidth\": 1},)\n",
    "ax.tick_params(axis='x', labelrotation=90, labelsize=5)\n",
    "# ax.set_ylim(0, ax.get_ylim()[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_fun = np.mean\n",
    "diffs = diffs_per_instance.groupby([\"compared\"]).apply(lambda x: agg_fun(x[\"distance\"]))\n",
    "diffs = diffs.sort_values()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "fig = plt.figure(figsize=(12,5), dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax = sns.barplot(x=diffs.index, y=diffs.values, ax=ax)\n",
    "ax.tick_params(axis='x', labelrotation=90, labelsize=5)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
