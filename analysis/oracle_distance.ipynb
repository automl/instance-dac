{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from rich import print as printr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from instance_dac.utils.data_loading import load_performance_data, load_eval_data\n",
    "\n",
    "# LEVELS OF AGGREGATION\n",
    "# 1. eval episode\n",
    "# 2. seed\n",
    "# 3. instance\n",
    "\n",
    "# data.csv missing? Run\n",
    "path = \"../runs/Sigmoid\"\n",
    "agent_name = \"ppo\"\n",
    "train_instance_set = \"2D3M_train\"\n",
    "train_instance_set_id = \"sigmoid_2D3M_train\"\n",
    "test_instance_set = \"2D3M_test\"\n",
    "test_instance_set_id = \"sigmoid_2D3M_test\"\n",
    "benchmark_id = \"Sigmoid\"\n",
    "\n",
    "path = \"../runs/CMA-ES\"\n",
    "agent_name = \"ppo_sb3\"\n",
    "train_instance_set = \"seplow_train\"\n",
    "test_instance_set_id = \"test\"\n",
    "train_instance_set_id = \"train\"\n",
    "test_instance_set = \"seplow_test\"\n",
    "benchmark_id = \"CMA-ES\"\n",
    "\n",
    "path = Path(path) / train_instance_set / agent_name\n",
    "# data = load_eval_data(path=path, instance_set_id=test_instance_set_id, instance_set=test_instance_set)\n",
    "\n",
    "data = pd.read_csv(f\"eval_data_{test_instance_set_id}.csv\")\n",
    "\n",
    "ids = data[\"origin\"] == \"selector\"\n",
    "data[\"origin\"][ids] = data[\"instance_set_id\"][ids].apply(lambda x: f\"selector_{x}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = data[\"origin\"].str.startswith(\"selector\")\n",
    "# Prepare to aggregate across runs and seeds\n",
    "data.loc[ids, \"origin\"] = data[\"instance_set_id\"][ids].apply(lambda x: \"selector__\" + \"__\".join(x.split(\"__\")[:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"origin\"].unique()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate across eval episodes\n",
    "D = []\n",
    "for gid, gdf in data.groupby([\"origin\", \"seed\", \"instance\", \"selector_run\"]):\n",
    "    if gid[0].startswith(\"selector\"):\n",
    "        D.append(pd.Series({\n",
    "            \"origin\": gid[0],\n",
    "            \"seed\": gid[1],\n",
    "            \"instance\": gid[2],\n",
    "            \"overall_performance\": gdf[\"overall_performance\"].mean()\n",
    "        }))\n",
    "    else:\n",
    "        print(gid)\n",
    "        D.append(gdf)\n",
    "perf = pd.concat(D, axis=1).T\n",
    "perf = pd.concat([perf, data[~data[\"origin\"].str.startswith(\"selector\")]])\n",
    "\n",
    "perf = pd.DataFrame(perf.groupby([\"origin\", \"seed\", \"instance\"])[\"overall_performance\"].mean())\n",
    "perf_dict = {}\n",
    "for gid, gdf in perf.groupby(\"origin\"):\n",
    "    gdf = gdf.reset_index()\n",
    "    P = gdf[\"overall_performance\"].to_numpy()\n",
    "    P = P.reshape((gdf[\"seed\"].nunique(), gdf[\"instance\"].nunique()))\n",
    "    perf_dict[gid] = P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_dict[\"selector__2D3M_train__Train__DS__Catch22__I__0.8\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rliable import library as rly\n",
    "from rliable import metrics\n",
    "from rliable import plot_utils\n",
    "\n",
    "metric_names = ['Median', 'IQM', 'Mean']#, 'Optimality Gap']\n",
    "\n",
    "algorithms = list(perf_dict.keys())\n",
    "# Load ALE scores as a dictionary mapping algorithms to their human normalized\n",
    "# score matrices, each of which is of size `(num_runs x num_games)`.\n",
    "aggregate_func = lambda x: np.array([\n",
    "  metrics.aggregate_median(x),\n",
    "  metrics.aggregate_iqm(x),\n",
    "  metrics.aggregate_mean(x),\n",
    "  #metrics.aggregate_optimality_gap(x)\n",
    "  ])\n",
    "aggregate_scores, aggregate_score_cis = rly.get_interval_estimates(\n",
    "  perf_dict, aggregate_func, reps=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'perf_dict_{benchmark_id}.pickle', 'wb') as handle:\n",
    "    pickle.dump(perf_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'aggregate_scores_{benchmark_id}.pickle', 'wb') as handle:\n",
    "    pickle.dump(aggregate_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'aggregate_score_cis_{benchmark_id}.pickle', 'wb') as handle:\n",
    "    pickle.dump(aggregate_score_cis, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_selector = \"selector_2D3M_train__Train__DS__Raw__A__0.7__9__2\"\n",
    "# algos = [a for a in algorithms if a in [\"full\", \"oracle\", \"random\", best_selector]]\n",
    "\n",
    "fig, axes = plot_utils.plot_interval_estimates(\n",
    "  aggregate_scores, aggregate_score_cis,\n",
    "  metric_names=metric_names,\n",
    "  algorithms=algorithms, xlabel='Performance')\n",
    "fig.savefig(f\"plot_{benchmark_id}.pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = []\n",
    "for key, value in aggregate_scores.items():\n",
    "    d = {\n",
    "        \"origin\": key,\n",
    "    }\n",
    "    d.update({m: v for m, v in zip(metric_names, value)})\n",
    "    D.append(d)\n",
    "df_scores = pd.DataFrame(D)\n",
    "\n",
    "# Highest IQM\n",
    "printr(\"Highest IQM\")\n",
    "ind = np.argpartition(df_scores[\"IQM\"], -2)[-2:]  # select 2nd best because best is oracle\n",
    "printr(df_scores.iloc[ind])\n",
    "printr(df_scores.iloc[ind][\"origin\"])\n",
    "\n",
    "# IQM Full\n",
    "printr(\"IQM full\")\n",
    "printr(df_scores[df_scores[\"origin\"] == \"full\"].iloc[0])\n",
    "\n",
    "\n",
    "# Sort by IQM\n",
    "df_scores = df_scores.sort_values(by=\"IQM\")\n",
    "\n",
    "df_scores.to_csv(f\"scores_{benchmark_id}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_list = np.linspace(1, 5, 21)\n",
    "score_distributions, score_distributions_cis = rly.create_performance_profile(\n",
    "    perf_dict, tau_list=tau_list)\n",
    "# Plot score distributions\n",
    "fig, ax = plt.subplots(ncols=1, figsize=(7, 5))\n",
    "plot_utils.plot_performance_profiles(\n",
    "  score_distributions, tau_list=tau_list,\n",
    "  performance_profile_cis=score_distributions_cis,\n",
    "  # colors=dict(zip(algorithms, sns.color_palette('colorblind'))),\n",
    "  xlabel=r'Overall Performance $(\\tau)$',\n",
    "  ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = pd.DataFrame(data.groupby([\"origin\", \"instance\"])[\"overall_performance\"].mean())\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def l1_dist(a, b) -> float:\n",
    "    return a - b \n",
    "\n",
    "def l2_dist(a, b) -> float:\n",
    "    return (a - b) ** 2\n",
    "\n",
    "def calc_dist(x: pd.DataFrame, distance_function) -> pd.DataFrame:\n",
    "    x = x.reset_index()\n",
    "    instance_id = x[\"instance\"][0]\n",
    "    origins = x[\"origin\"]\n",
    "    groundtruth = \"oracle\"\n",
    "    idx_gt = list(origins).index(groundtruth)\n",
    "    dist = distance_function(x[\"overall_performance\"][idx_gt], x[\"overall_performance\"])\n",
    "    comparison_names = [f\"{groundtruth} - {origin}\" for origin in origins]\n",
    "    ret = pd.DataFrame({\n",
    "        \"instance\": instance_id,\n",
    "        \"distance_name\": distance_function.__name__,\n",
    "        \"distance\": dist,\n",
    "        \"compared\": comparison_names\n",
    "    })\n",
    "    return ret\n",
    "\n",
    "distance_functions=[l1_dist, l2_dist]\n",
    "\n",
    "# Compute distance between oracle performance and performance on full training set\n",
    "diffs_per_instance = pd.concat([perf.groupby(\"instance\").apply(calc_dist, distance_function=func) for func in distance_functions], axis=0).reset_index(drop=True)\n",
    "diffs_per_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort df by agg_fun\n",
    "agg_fun = np.mean\n",
    "\n",
    "sorted_series = diffs_per_instance.groupby([\"compared\"]).apply(lambda x: agg_fun(x[\"distance\"])).sort_values()\n",
    "sorter = list(sorted_series.index)\n",
    "\n",
    "df = diffs_per_instance.sort_values(by=\"compared\", key=lambda column: column.map(lambda e: sorter.index(e)))\n",
    "df = df[df[\"distance_name\"] == \"l2_dist\"]\n",
    "\n",
    "fig = plt.figure(figsize=(12,5), dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax = sns.boxplot(data=df, x=\"compared\", y=\"distance\", ax=ax, fliersize=2)\n",
    "ax.tick_params(axis='x', labelrotation=90, labelsize=5)\n",
    "# ax.set_ylim(0, ax.get_ylim()[1])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(12,5), dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax = sns.barplot(data=df, x=\"compared\", y=\"distance\", ax=ax) #, err_kws={\"color\": \".5\", \"linewidth\": 1},)\n",
    "ax.tick_params(axis='x', labelrotation=90, labelsize=5)\n",
    "# ax.set_ylim(0, ax.get_ylim()[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_fun = np.mean\n",
    "diffs = diffs_per_instance.groupby([\"compared\"]).apply(lambda x: agg_fun(x[\"distance\"]))\n",
    "diffs = diffs.sort_values()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "fig = plt.figure(figsize=(12,5), dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax = sns.barplot(x=diffs.index, y=diffs.values, ax=ax)\n",
    "ax.tick_params(axis='x', labelrotation=90, labelsize=5)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
